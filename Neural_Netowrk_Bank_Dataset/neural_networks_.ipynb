{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbM+kGa9lYu+dhHlw/jHbj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dabster108/Neural-Networks-Scratch/blob/main/neural_networks_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "xHw7m0eUALdJ",
        "outputId": "5bb3619c-2d4a-4589-c958-f4f16596a13f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Neural network is a computational system inspired by the human brain, called an artificial neural network (ANN).It is used to learn patterns from data and make predictions.\\n\\nLayers\\n\\t1.\\tInput layer → receives the data (like numbers, images, or words).\\n\\t2.\\tHidden layer(s) → processes the data and finds patterns.\\n\\t3.\\tOutput layer → produces the result or prediction.\\n\\nKey Terms\\n\\t•\\tWeight → decides how much influence an input has on the output.\\n\\t•\\tBias → a number added to input to adjust the output and help fit the data better.\\n\\t•\\tGradient → the signal used to update weights and biases during training so the network learns. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "''' Neural network is a computational system inspired by the human brain, called an artificial neural network (ANN).It is used to learn patterns from data and make predictions.\n",
        "\n",
        "Layers\n",
        "\t1.\tInput layer → receives the data (like numbers, images, or words).\n",
        "\t2.\tHidden layer(s) → processes the data and finds patterns.\n",
        "\t3.\tOutput layer → produces the result or prediction.\n",
        "\n",
        "Key Terms\n",
        "\t•\tWeight → decides how much influence an input has on the output.\n",
        "\t•\tBias → a number added to input to adjust the output and help fit the data better.\n",
        "\t•\tGradient → the signal used to update weights and biases during training so the network learns. '''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' Step 1: Import the necessary libraries (NumPy for computations).\n",
        "Step 2: Define the network architecture, including input size, hidden layers, and output size. Initialize weights and biases randomly.\n",
        "Step 3: Define the activation functions for the hidden and output layers along with their derivatives for backpropagation.\n",
        "Step 4: Implement forward propagation to compute outputs from inputs through the network.\n",
        "Step 5: Compute the loss using a suitable loss function, such as binary cross-entropy for binary classification.\n",
        "Step 6: Implement backward propagation to calculate gradients and update the weights and biases using gradient descent.\n",
        "Step 7: Set up a training loop to iterate the forward and backward passes over multiple epochs while monitoring the loss.\n",
        "Step 8: Create a prediction function that applies a threshold on the output to classify inputs.\n",
        "Step 9: Test the network on a dataset like XOR to ensure it learns correctly.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "00wZy24jAfbN",
        "outputId": "8faadb45-4df7-4524-c235-2bb3216e2851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Step 1: Import the necessary libraries (NumPy for computations).\\nStep 2: Define the network architecture, including input size, hidden layers, and output size. Initialize weights and biases randomly.\\nStep 3: Define the activation functions for the hidden and output layers along with their derivatives for backpropagation.\\nStep 4: Implement forward propagation to compute outputs from inputs through the network.\\nStep 5: Compute the loss using a suitable loss function, such as binary cross-entropy for binary classification.\\nStep 6: Implement backward propagation to calculate gradients and update the weights and biases using gradient descent.\\nStep 7: Set up a training loop to iterate the forward and backward passes over multiple epochs while monitoring the loss.\\nStep 8: Create a prediction function that applies a threshold on the output to classify inputs.\\nStep 9: Test the network on a dataset like XOR to ensure it learns correctly.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# For our first example, we’ll build a small network that can learn the XOR problem:\n",
        "# Input layer → 2 neurons (since XOR has 2 inputs)\n",
        "# Hidden layer → 2 neurons\n",
        "# Output layer → 1 neuron (binary classification: 0 or 1)\n",
        "\n",
        "'''\n",
        "So, basically weights are learnable parameters for which the input has the influence in the ouput and biases are like parameter or added numbers to fit data better\n",
        "'''"
      ],
      "metadata": {
        "id": "avrnaub3AmA-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6c86805f-ace5-47a6-fdaf-888bd94afc21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' \\nSo, basically weights are learnable parameters for which the input has the influence in the ouput and biases are like parameter or added numbers to fit data better \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Network architecture\n",
        "input_size = 2\n",
        "hidden_size = 2\n",
        "output_size = 1\n",
        "\n",
        "# Initialize weights and biases\n",
        "W1 = np.random.randn(input_size, hidden_size)   # weights for input -> hidden\n",
        "b1 = np.zeros((1, hidden_size))                 # biases for hidden layer\n",
        "W2 = np.random.randn(hidden_size, output_size)  # weights for hidden -> output\n",
        "b2 = np.zeros((1, output_size))                 # biases for output layer"
      ],
      "metadata": {
        "id": "QNVBxp_IqcEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Activation functions introduce non-linearity into the network, which is crucial because without them, the network would just be doing linear math (like a straight line) acouldn’t learn complex patterns like XOR.\n",
        "We will use 2 activation functions :\n",
        "RELU (Rectified Linear Unit) for hidden layers\n",
        "Sigmoid Function  (Since we need problabilities for the binary classification )\n",
        "'''\n",
        "# Sigmoid activation (for output layer)\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Derivative of sigmoid (for backpropagation)\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# ReLU activation (for hidden layer)\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Derivative of ReLU (for backpropagation)\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)"
      ],
      "metadata": {
        "id": "G3wyR6Pwqc81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Forward Propagation means passing the input through the network (layer by layer) to get the ouput\n",
        "Multiply input by weights and add bias (linear part)\n",
        "Apply activation function (non-linear part)\n",
        "Pass result to the next layer\n",
        "'''\n",
        "def forward(X):\n",
        "    # Input -> Hidden\n",
        "    Z1 = np.dot(X, W1) + b1     # linear step\n",
        "    A1 = relu(Z1)               # apply ReLU\n",
        "\n",
        "    # Hidden -> Output\n",
        "    Z2 = np.dot(A1, W2) + b2    # linear step\n",
        "    A2 = sigmoid(Z2)            # apply Sigmoid (probabilities)\n",
        "\n",
        "    return Z1, A1, Z2, A2\n",
        "\n"
      ],
      "metadata": {
        "id": "GJdT_M7UrS5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Now we use the loss function to evaluate the networks prediction from the actual target values '''\n",
        "def compute_loss(y_true, y_pred):\n",
        "    m = y_true.shape[0]  # number of samples\n",
        "    loss = - (1/m) * np.sum(\n",
        "        y_true * np.log(y_pred + 1e-8) + (1 - y_true) * np.log(1 - y_pred + 1e-8)\n",
        "    )\n",
        "    return loss"
      ],
      "metadata": {
        "id": "7JkMe3X_sDFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Backward Propagation\n",
        "Backward propagation (backprop) is where the network learns by adjusting its weights and biases.\n",
        "We calculate the gradient of the loss with respect to each parameter (using the chain rule) and update them with gradient descent.\n",
        "\n",
        "'''\n",
        "def backward(X, y, Z1, A1, Z2, A2, W1, b1, W2, b2, learning_rate=0.1):\n",
        "    m = y.shape[0]\n",
        "\n",
        "    # Output layer error\n",
        "    dZ2 = A2 - y\n",
        "    dW2 = (1/m) * np.dot(A1.T, dZ2)\n",
        "    db2 = (1/m) * np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "    # Hidden layer error\n",
        "    dA1 = np.dot(dZ2, W2.T)\n",
        "    dZ1 = dA1 * relu_derivative(Z1)\n",
        "    dW1 = (1/m) * np.dot(X.T, dZ1)\n",
        "    db1 = (1/m) * np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update parameters\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "\n",
        "    return W1, b1, W2, b2"
      ],
      "metadata": {
        "id": "vHseI8iQsZZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Traning the neural networks\n",
        "Now we’ll train the neural network by repeatedly doing:\n",
        "\t1.\tForward propagation → get predictions\n",
        "\t2.\tCompute loss → see how wrong the predictions are\n",
        "\t3.\tBackward propagation → update weights and biases\n",
        " '''\n",
        " # XOR dataset\n",
        "X = np.array([[0,0],\n",
        "              [0,1],\n",
        "              [1,0],\n",
        "              [1,1]])\n",
        "y = np.array([[0],\n",
        "              [1],\n",
        "              [1],\n",
        "              [0]])\n",
        "\n",
        "epochs = 10000\n",
        "learning_rate = 0.1\n",
        "\n",
        "for i in range(epochs):\n",
        "    # Forward pass\n",
        "    Z1, A1, Z2, A2 = forward(X)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = compute_loss(y, A2)\n",
        "\n",
        "    # Backward pass (update weights and biases)\n",
        "    W1, b1, W2, b2 = backward(X, y, Z1, A1, Z2, A2, W1, b1, W2, b2, learning_rate)\n",
        "\n",
        "    # Print loss occasionally\n",
        "    if i % 1000 == 0:\n",
        "        print(f\"Epoch {i}, Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-mv17jtshLP",
        "outputId": "53d80237-de82-4b05-b98b-f28d604098e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.7164\n",
            "Epoch 1000, Loss: 0.3533\n",
            "Epoch 2000, Loss: 0.3486\n",
            "Epoch 3000, Loss: 0.3477\n",
            "Epoch 4000, Loss: 0.3474\n",
            "Epoch 5000, Loss: 0.3472\n",
            "Epoch 6000, Loss: 0.3470\n",
            "Epoch 7000, Loss: 0.3469\n",
            "Epoch 8000, Loss: 0.3469\n",
            "Epoch 9000, Loss: 0.3468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' Prediction Function '''\n",
        "def predict(X, W1, b1, W2, b2):\n",
        "    _, _, _, A2 = forward(X)\n",
        "    return (A2 > 0.5).astype(int)\n",
        "print(\"Predictions:\")\n",
        "print(predict(X, W1, b1, W2, b2))\n",
        "print(\"Actual:\")\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPcLiz3hs2kT",
        "outputId": "e32b0cbc-2f03-4041-96ac-5a85c19eec0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:\n",
            "[[0]\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "Actual:\n",
            "[[0]\n",
            " [1]\n",
            " [1]\n",
            " [0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8O6Fz1zbtttS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}